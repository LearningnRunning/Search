"""
ìŒì‹ì  ê²€ìƒ‰ ì—”ì§„
"""

import logging
from typing import Any, Optional

import pandas as pd
import torch
from sentence_transformers import SentenceTransformer, util

from src.utils import jamo_similarity, normalize
from src.embedding_loader import EmbeddingLoader, load_embeddings

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)


class SemanticSearcher:
    """ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ì„ ìœ„í•œ í´ëž˜ìŠ¤"""

    def __init__(self, model_name: str = "snunlp/KR-SBERT-V40K-klueNLI-augSTS"):
        """
        Args:
            model_name: ì‚¬ìš©í•  SBERT ëª¨ë¸ëª…
        """
        try:
            self.model = SentenceTransformer(model_name)
            # ë””ë°”ì´ìŠ¤ ì •ë³´ ë¡œê¹…
            device = next(self.model.parameters()).device
            logger.info(f"SBERT ëª¨ë¸ì´ {device}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"SBERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("ë¡œì»¬ ëª¨ë¸ ë˜ëŠ” ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            # ê°„ë‹¨í•œ ë¡œì»¬ ëª¨ë¸ ë˜ëŠ” Noneìœ¼ë¡œ ì„¤ì •
            self.model = None

    def similarity(
        self, query: str, candidates: list[str], top_k: int = 5
    ) -> list[tuple[str, float]]:
        """
        ì¿¼ë¦¬ì™€ í›„ë³´ë“¤ ê°„ì˜ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            candidates: í›„ë³´ ë¬¸ìžì—´ë“¤
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜

        Returns:
            (í›„ë³´, ì ìˆ˜) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
        """
        if self.model is None:
            # ëª¨ë¸ì´ ì—†ì„ ë•ŒëŠ” ê°„ë‹¨í•œ ë¬¸ìžì—´ ë§¤ì¹­ìœ¼ë¡œ ëŒ€ì²´
            logger.warning("SBERT ëª¨ë¸ì´ ì—†ì–´ ê°„ë‹¨í•œ ë¬¸ìžì—´ ë§¤ì¹­ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            results = []
            for candidate in candidates:
                # ê°„ë‹¨í•œ í¬í•¨ ê´€ê³„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
                if query.lower() in candidate.lower():
                    score = len(query) / len(candidate)
                    results.append((candidate, score))
            return sorted(results, key=lambda x: x[1], reverse=True)[:top_k]
        
        query_embedding = self.model.encode(query, convert_to_tensor=True)
        candidate_embeddings = self.model.encode(candidates, convert_to_tensor=True)
        cos_scores = util.cos_sim(query_embedding, candidate_embeddings)[0]
        top_results = torch.topk(cos_scores, k=top_k)
        return [
            (candidates[idx], round(score.item(), 4))
            for idx, score in zip(top_results.indices, top_results.values, strict=False)
        ]


class DinerSearchEngine:
    """ìŒì‹ì  ê²€ìƒ‰ ì—”ì§„"""

    def __init__(
        self,
        diner_infos: Optional[list[dict[str, Any]]] = None,
        model_name: str = "snunlp/KR-SBERT-V40K-klueNLI-augSTS",
        embeddings_dir: str = "data/embeddings",
        use_precomputed_embeddings: bool = True
    ):
        """
        Args:
            diner_infos: ìŒì‹ì  ì •ë³´ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ë²¡í„° íŒŒì¼ì—ì„œ ë¡œë“œ)
            model_name: SBERT ëª¨ë¸ëª…
            embeddings_dir: ë²¡í„° íŒŒì¼ë“¤ì´ ì €ìž¥ëœ ë””ë ‰í† ë¦¬
            use_precomputed_embeddings: ë¯¸ë¦¬ ê³„ì‚°ëœ ë²¡í„°ë¥¼ ì‚¬ìš©í• ì§€ ì—¬ë¶€
        """
        self.model_name = model_name
        self.embeddings_dir = embeddings_dir
        self.use_precomputed_embeddings = use_precomputed_embeddings
        
        # ë²¡í„° ë¡œë” ì´ˆê¸°í™”
        self.embedding_loader = None
        self.diner_embeddings = None
        
        if diner_infos is not None:
            self.diner_infos = diner_infos
        else:
            self.diner_infos = None
        
        # semantic_searcherëŠ” í•­ìƒ ì´ˆê¸°í™” (ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ì— í•„ìš”)
        logger.info("ðŸ¤– SBERT ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.semantic_searcher = SemanticSearcher(self.model_name)
        
        # ë¯¸ë¦¬ ê³„ì‚°ëœ ë²¡í„° ì‚¬ìš© ì‹œë„
        if use_precomputed_embeddings:
            self._load_precomputed_embeddings()
        
        # ë²¡í„° ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        if self.diner_embeddings is None:
            self._initialize_with_model()
        
        # ë””ë°”ì´ìŠ¤ í†µì¼ í™•ì¸
        self._ensure_device_consistency()
    
    def _ensure_device_consistency(self):
        """ëª¨ë“  í…ì„œê°€ ê°™ì€ ë””ë°”ì´ìŠ¤ì— ìžˆëŠ”ì§€ í™•ì¸í•˜ê³  í†µì¼í•©ë‹ˆë‹¤."""
        if self.diner_embeddings is not None:
            model_device = next(self.semantic_searcher.model.parameters()).device
            if self.diner_embeddings.device != model_device:
                logger.info(f"í…ì„œ ë””ë°”ì´ìŠ¤ í†µì¼: {self.diner_embeddings.device} -> {model_device}")
                self.diner_embeddings = self.diner_embeddings.to(model_device)
    
    def _load_precomputed_embeddings(self):
        """ë¯¸ë¦¬ ê³„ì‚°ëœ ë²¡í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            logger.info("ðŸ”„ ë¯¸ë¦¬ ê³„ì‚°ëœ ë²¡í„° ë¡œë“œ ì¤‘...")
            self.embedding_loader = load_embeddings(self.embeddings_dir)
            
            if self.embedding_loader and self.embedding_loader.is_loaded():
                self.diner_embeddings = self.embedding_loader.get_embeddings()
                if self.diner_infos is None:
                    self.diner_infos = self.embedding_loader.get_diner_infos()
                
                info = self.embedding_loader.get_info()
                logger.info(f"âœ… ë¯¸ë¦¬ ê³„ì‚°ëœ ë²¡í„° ë¡œë“œ ì™„ë£Œ!")
                logger.info(f"   ìŒì‹ì  ìˆ˜: {info['num_diners']:,}ê°œ")
                logger.info(f"   ë²¡í„° ì°¨ì›: {info['embedding_dim']}")
                logger.info(f"   íŒŒì¼ í¬ê¸°: {info['file_size_mb']:.1f}MB")
                logger.info(f"   ëª¨ë¸: {info['model_name']}")
            else:
                logger.warning("âš ï¸ ë¯¸ë¦¬ ê³„ì‚°ëœ ë²¡í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.info("ðŸ”„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
    
    def _initialize_with_model(self):
        """ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        logger.info("ðŸ¤– SBERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì´ˆê¸°í™” ì¤‘...")
        
        if self.diner_infos is None:
            from src.utils import load_diner_data
            self.diner_infos = load_diner_data()
        
        # ëª¨ë¸ì´ ì—†ì„ ë•ŒëŠ” ë²¡í„° ì´ˆê¸°í™”ë¥¼ ê±´ë„ˆëœ€
        if self.semantic_searcher.model is None:
            logger.warning("âš ï¸ SBERT ëª¨ë¸ì´ ì—†ì–´ ë²¡í„° ì´ˆê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            self.diner_embeddings = None
            return
        
        # ìŒì‹ì  ì´ë¦„ë“¤ì˜ ìž„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°
        diner_names = [d["name"] for d in self.diner_infos]
        logger.info(f"ðŸ”¢ {len(diner_names)}ê°œ ìŒì‹ì ì˜ ë²¡í„° ìƒì„± ì¤‘...")
        self.diner_embeddings = self.semantic_searcher.model.encode(
            diner_names, convert_to_tensor=True
        )
        logger.info("âœ… ë²¡í„° ìƒì„± ì™„ë£Œ!")

    def search(
        self, query: str, top_k: int = 5, jamo_threshold: float = 0.9
    ) -> pd.DataFrame:
        """
        ìŒì‹ì ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ DataFrame
        """
        norm_query = normalize(query)

        # 1. ì •í™•í•œ ë§¤ì¹­
        exact_matches = [
            d for d in self.diner_infos if normalize(d["name"]) == norm_query
        ]
        if exact_matches:
            results = pd.DataFrame(exact_matches).assign(match_type="ì •í™•í•œ ë§¤ì¹­")
            return self._add_kakao_map_links(results)

        # 2. ë¶€ë¶„ ë§¤ì¹­
        partial_matches = [
            d for d in self.diner_infos if norm_query in normalize(d["name"])
        ]
        if partial_matches:
            results = pd.DataFrame(partial_matches).assign(match_type="ë¶€ë¶„ ë§¤ì¹­")
            return self._add_kakao_map_links(results)

        # 3. ìžëª¨ ê¸°ë°˜ ì§ì ‘ ë§¤ì¹­
        for d in self.diner_infos:
            is_jamo, score = jamo_similarity(norm_query, normalize(d["name"]))
            if is_jamo:
                results = pd.DataFrame(
                    [
                        {
                            "name": d["name"],
                            "idx": d["idx"],
                            "jamo_score": score,
                            "match_type": "ìžëª¨ ë§¤ì¹­",
                        }
                    ]
                )
                return self._add_kakao_map_links(results)

        # 4. ìžëª¨ ìœ ì‚¬ë„ ì „ì²´ ê³„ì‚° (ê¸°ë³¸ ìž„ê³„ê°’ 0.7 ì‚¬ìš©)
        jamo_scores = []
        for d in self.diner_infos:
            _, score = jamo_similarity(norm_query, normalize(d["name"]))
            jamo_scores.append((d, score))

        jamo_top = sorted(
            [x for x in jamo_scores if x[1] >= jamo_threshold],
            key=lambda x: x[1],
            reverse=True,
        )[:top_k]

        # 5. SBERT ì˜ë¯¸ë¡ ì  ê²€ìƒ‰
        sbert_top = []
        if self.semantic_searcher.model is not None and self.diner_embeddings is not None:
            try:
                # ì¿¼ë¦¬ ìž„ë² ë”© ìƒì„±
                query_emb = self.semantic_searcher.model.encode(query, convert_to_tensor=True)
                
                # ë””ë°”ì´ìŠ¤ í†µì¼: diner_embeddingsë¥¼ query_embì™€ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                if self.diner_embeddings.device != query_emb.device:
                    logger.info(f"ê²€ìƒ‰ ì¤‘ í…ì„œ ë””ë°”ì´ìŠ¤ í†µì¼: {self.diner_embeddings.device} -> {query_emb.device}")
                    self.diner_embeddings = self.diner_embeddings.to(query_emb.device)
                
                cos_scores = util.cos_sim(query_emb, self.diner_embeddings)[0]
                topk = torch.topk(cos_scores, k=top_k)
                sbert_top = [
                    (self.diner_infos[idx], round(score.item(), 4))
                    for idx, score in zip(topk.indices.cpu(), topk.values.cpu(), strict=False)
                ]
                
            except Exception as e:
                logger.error(f"ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                sbert_top = []
        else:
            logger.info("SBERT ëª¨ë¸ì´ ì—†ì–´ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ìžëª¨ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜
        if not sbert_top and jamo_top:
            results = pd.DataFrame([
                {
                    "name": d["name"],
                    "idx": d["idx"],
                    "score": score,
                    "match_type": "ìžëª¨ ë§¤ì¹­",
                }
                for d, score in jamo_top
            ])
            return self._add_kakao_map_links(results)
        elif not sbert_top and not jamo_top:
            return pd.DataFrame().assign(match_type="ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

        # 6. ì ìˆ˜ í•©ì‚° (í†µí•© ê²€ìƒ‰)
        combined = {}
        for d, score in jamo_top:
            key = d["name"]
            combined[key] = {"name": key, "idx": d["idx"], "score": score * 0.5}

        for d, score in sbert_top:
            key = d["name"]
            if key in combined:
                combined[key]["score"] += score * 0.5
            else:
                combined[key] = {"name": key, "idx": d["idx"], "score": score * 0.5}

        results = pd.DataFrame(
            sorted(combined.values(), key=lambda x: x["score"], reverse=True)[:top_k]
        ).assign(match_type="í†µí•© ê²€ìƒ‰")
        
        return self._add_kakao_map_links(results)
    
    def _add_kakao_map_links(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê²€ìƒ‰ ê²°ê³¼ì— ì¹´ì¹´ì˜¤ë§µ ë§í¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        if df.empty:
            return df
        
        # ì¹´ì¹´ì˜¤ë§µ ë§í¬ ì»¬ëŸ¼ ì¶”ê°€
        df = df.copy()
        # name ì»¬ëŸ¼ì„ ì¹´ì¹´ì˜¤ë§µ ë§í¬ë¡œ ë³€í™˜
        df['name'] = df.apply(lambda row: f"[{row['name']}](https://place.map.kakao.com/{row['idx']})", axis=1)
        
        return df
    
    def get_info(self) -> dict:
        """ê²€ìƒ‰ ì—”ì§„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        info = {
            "model_name": self.model_name,
            "num_diners": len(self.diner_infos),
            "use_precomputed_embeddings": self.use_precomputed_embeddings
        }
        
        if self.embedding_loader:
            info.update(self.embedding_loader.get_info())
        
        return info
